{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "224c6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "329a4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.randint(0, 255, (10000, 10000, 21), dtype=torch.uint8)\n",
    "x = np.random.randint(0, 255, (10000, 10000, 21), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7631f6f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mTF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh w c -> c h w\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/estorrs/miniconda3/envs/dino_extended/lib/python3.9/site-packages/torchvision/transforms/functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    471\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    472\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    474\u001b[0m         )\n\u001b[0;32m--> 476\u001b[0m _, image_height, image_width \u001b[38;5;241m=\u001b[39m \u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    478\u001b[0m     size \u001b[38;5;241m=\u001b[39m [size]\n",
      "File \u001b[0;32m/data/estorrs/miniconda3/envs/dino_extended/lib/python3.9/site-packages/torchvision/transforms/functional.py:78\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/estorrs/miniconda3/envs/dino_extended/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m     29\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "y = TF.resize(rearrange(x, 'h w c -> c h w'), (1000, 1000), antialias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c20aa4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[136, 129, 118,  ..., 139, 130, 130],\n",
       "         [137, 131, 131,  ..., 133, 126, 126],\n",
       "         [134, 122, 122,  ..., 120, 122, 120],\n",
       "         ...,\n",
       "         [128, 123, 125,  ..., 121, 128, 128],\n",
       "         [140, 125, 121,  ..., 131, 128, 132],\n",
       "         [129, 124, 119,  ..., 129, 127, 130]],\n",
       "\n",
       "        [[128, 126, 128,  ..., 124, 124, 120],\n",
       "         [134, 137, 120,  ..., 135, 128, 128],\n",
       "         [131, 132, 124,  ..., 130, 126, 122],\n",
       "         ...,\n",
       "         [127, 131, 122,  ..., 138, 133, 122],\n",
       "         [129, 125, 128,  ..., 129, 136, 133],\n",
       "         [133, 128, 128,  ..., 129, 128, 128]],\n",
       "\n",
       "        [[129, 132, 135,  ..., 133, 132, 138],\n",
       "         [132, 128, 133,  ..., 131, 126, 127],\n",
       "         [127, 121, 119,  ..., 130, 129, 125],\n",
       "         ...,\n",
       "         [131, 126, 126,  ..., 126, 129, 133],\n",
       "         [122, 124, 127,  ..., 126, 116, 138],\n",
       "         [121, 135, 124,  ..., 127, 124, 132]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[124, 124, 130,  ..., 114, 124, 125],\n",
       "         [127, 129, 122,  ..., 124, 134, 132],\n",
       "         [124, 131, 130,  ..., 128, 127, 135],\n",
       "         ...,\n",
       "         [133, 129, 122,  ..., 124, 127, 134],\n",
       "         [125, 119, 132,  ..., 130, 120, 128],\n",
       "         [107, 125, 136,  ..., 128, 125, 130]],\n",
       "\n",
       "        [[127, 123, 121,  ..., 115, 125, 125],\n",
       "         [119, 129, 129,  ..., 129, 134, 133],\n",
       "         [120, 125, 126,  ..., 128, 134, 133],\n",
       "         ...,\n",
       "         [133, 134, 129,  ..., 134, 128, 127],\n",
       "         [123, 128, 124,  ..., 132, 128, 131],\n",
       "         [134, 128, 119,  ..., 130, 125, 124]],\n",
       "\n",
       "        [[116, 118, 126,  ..., 130, 122, 135],\n",
       "         [128, 125, 134,  ..., 127, 133, 134],\n",
       "         [121, 138, 135,  ..., 130, 130, 127],\n",
       "         ...,\n",
       "         [122, 120, 135,  ..., 133, 122, 123],\n",
       "         [125, 127, 132,  ..., 124, 128, 133],\n",
       "         [126, 128, 125,  ..., 133, 136, 138]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c21eba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19eb912e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 7, 5, 7, 9, 9, 2, 6, 8, 6],\n",
       "        [9, 5, 8, 2, 7, 6, 9, 5, 6, 1],\n",
       "        [6, 9, 7, 4, 1, 8, 7, 7, 2, 1],\n",
       "        [7, 3, 1, 2, 2, 7, 9, 5, 9, 4],\n",
       "        [3, 1, 5, 5, 8, 5, 5, 5, 5, 8],\n",
       "        [5, 7, 7, 2, 4, 8, 7, 4, 0, 7],\n",
       "        [1, 6, 0, 5, 6, 0, 9, 6, 6, 1],\n",
       "        [5, 9, 3, 8, 1, 8, 8, 0, 6, 1],\n",
       "        [6, 3, 6, 9, 8, 7, 3, 6, 1, 9],\n",
       "        [5, 8, 4, 9, 7, 2, 2, 6, 8, 4]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.random.randint(0, 10, (10, 10), dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb81a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
